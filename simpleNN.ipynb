{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple nerual network for fun!\n",
    "\n",
    "### Definations:\n",
    "\n",
    "$N_n$: The number of nodes in $n$-$th$ layer.\n",
    "\n",
    "$X$: The input of ANN, equals to $O^{(1)}$. Shape: ($M$, $N_1$)\n",
    "\n",
    "$O^{(n)}$: The output of $n$-$th$ layer, also the input of $(n+1)$-$th$ layer. Shape: $(M,N_n)$\n",
    "\n",
    "$W^{(n)}$: The weight of $n$-$th$ layer. Shape: $(N_n, N_{n+1})$\n",
    "\n",
    "$B^{(n)}$: The bias of $n$-$th$ layer. Shape: $(N_{n+1},)$\n",
    "\n",
    "$A^{(n)}$: The intermediate value in each layer, equals to $O^{(n)}W^{(n)}+B^{(n)}$. Shape: $(M, N_{n+1})$\n",
    "\n",
    "### Euqaitons:\n",
    " * Forward Propagation\n",
    "\\begin{align}\n",
    "    O^{(n+1)} & = Active(A^{(n)}) \\\\\n",
    "              & = Active(O^{(n)}W^{(n)}+B^{(n)}) \\\\\n",
    "\\end{align}\n",
    "\n",
    " * Backward Propagation\n",
    "\\begin{align}\n",
    "    \\frac{\\partial E}{\\partial A^{(n)}} &= \\frac{\\partial E}{\\partial O^{(n+1)}} \n",
    "    \\odot Active^\\prime(A^{(n)}) \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial E}{\\partial O^{(n)}} &= \\frac{\\partial E}{\\partial A^{(n)}} {W^{(n)}}^T \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial E}{\\partial W^{(n)}} &= {O^{(n)}}^T\\frac{\\partial E}{\\partial A^{(n)}} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial E}{\\partial B^{(n)}} &= \\sum_{m}\\big( \\frac{\\partial E}{\\partial A^{(n)}} \\big)_{m}\n",
    "\\end{align}\n",
    "\n",
    "*Matrix derivative uses denominator alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "delta = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "def label2bin(y):\n",
    "    shape = (y.shape[0], np.max(y) + 1)\n",
    "    rst = np.zeros(shape, dtype = int)\n",
    "    for idx, label in enumerate(y):\n",
    "        rst[idx, label] = 1\n",
    "    return rst\n",
    "\n",
    "def maxCol(X):\n",
    "    return np.max(X, axis = 1)[:, np.newaxis]\n",
    "\n",
    "def sumCol(X):\n",
    "    return np.sum(X, axis = 1)[:, np.newaxis]\n",
    "\n",
    "def averCol(X):\n",
    "    return np.average(X, axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pairs of math functions.\n",
    "f(x) returns the values for forward propagation. \n",
    "Df(x, dr) applies chain rule and returns derivatives of parameters for backward propagation.\n",
    "\"\"\"\n",
    "def mul(X, Y):\n",
    "    return X.dot(Y)\n",
    "\n",
    "def Dmul(X, Y, dLdO):\n",
    "    return dLdO.dot(Y.T), X.T.dot(dLdO)\n",
    "\n",
    "def relu(A):\n",
    "    return A * (A > 0)\n",
    "\n",
    "def Drelu(A, dLdO, O = None):\n",
    "    return dLdO * (A > 0)\n",
    "\n",
    "def sigmoid(A):\n",
    "    y = 1 / (1 + np.exp(-A))\n",
    "    return y\n",
    "\n",
    "def Dsigmoid(A, dLdO, O = None):\n",
    "    if O is None:\n",
    "        O = sigmoid(A)\n",
    "    return dLdO * O * (1 - O)\n",
    "\n",
    "def softMax(A):\n",
    "    shift = A - maxCol(A)\n",
    "    exp = np.exp(shift)\n",
    "    return exp / sumCol(exp)\n",
    "\n",
    "def DsoftMax(A, dLdO, O = None):\n",
    "    if O is None:\n",
    "        O = softMax(A)\n",
    "    O_dLdO = O * dLdO\n",
    "    return O_dLdO - O * sumCol(O_dLdO)\n",
    "\n",
    "activFunc = {\n",
    "    'relu': relu,\n",
    "    'sigmoid': sigmoid,\n",
    "    'softMax': softMax,\n",
    "}\n",
    "\n",
    "DactivFunc = {\n",
    "    'relu': Drelu,\n",
    "    'sigmoid': Dsigmoid,\n",
    "    'softMax': DsoftMax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss functions.\n",
    "    inputs(\n",
    "        O: Output layer\n",
    "        y: True value\n",
    "    )\n",
    "    return(\n",
    "        loss: Loss of the neural network.(without regularization)\n",
    "        dLdO: Derivative of loss with respect of O\n",
    "        dLdA: Derivative of loss with respect of A\n",
    "    )\n",
    "\"\"\"\n",
    "def LSM(O, y):\n",
    "    assert(O.shape == y.shape)\n",
    "    M = O.shape[0]\n",
    "    dLdO = O - y\n",
    "    loss = np.sum(dLdO**2) / (2 * M)\n",
    "    return loss, dLdO, None\n",
    "\n",
    "# y is class label.\n",
    "def labelCrossEntropy(O, y):\n",
    "    M = O.shape[0]\n",
    "    dLdO = -label2bin(y) / (O + delta)\n",
    "    loss = -np.sum(np.log(np.choose(y, O.T) + delta)) / M\n",
    "    return loss, dLdO, None\n",
    "\n",
    "# y is class probability.\n",
    "def probCrossEntropy(O, y):\n",
    "    assert(O.shape == y.shape)\n",
    "    M = O.shape[0]\n",
    "    dLdO = -y / (O + delta)\n",
    "    loss = -np.sum(np.log(O + delta) * y) / M\n",
    "    return loss, dLdO, None\n",
    "\n",
    "# Output layer is softMax\n",
    "def softMaxLabelCrossEntropy(O, y):\n",
    "    M = O.shape[0]\n",
    "    loss = -np.sum(np.log(np.choose(y, O.T) + delta)) / M\n",
    "    return loss, None, (O - label2bin(y))\n",
    "\n",
    "def softMaxProbCrossEntropy(O, y):\n",
    "    assert(O.shape == y.shape)\n",
    "    loss = -np.sum(np.log(O + delta) * y) / M\n",
    "    return loss, None, (O - y)\n",
    "\n",
    "lossFunc = {\n",
    "    'LSM': LSM,\n",
    "    'labelCrossEntropy': labelCrossEntropy,\n",
    "    'probCrossEntropy': probCrossEntropy,\n",
    "    'softMaxLabelCrossEntropy': softMaxLabelCrossEntropy,\n",
    "    'softMaxProbCrossEntropy': softMaxProbCrossEntropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A layer of nerual network\n",
    "\"\"\"\n",
    "class layer:\n",
    "    def __init__(self, shape, activ, isOutLayer = False, loss = None, reg = 1e-1, std = 1e-1):\n",
    "        assert(len(shape) == 2)\n",
    "        self.W = std * np.random.randn(*shape)\n",
    "        self.B = np.zeros(shape[1])\n",
    "        self.reg = reg\n",
    "        self.activ = activFunc[activ]\n",
    "        self.Dactiv = DactivFunc[activ]\n",
    "        self.isOutLayer = isOutLayer\n",
    "        if isOutLayer:\n",
    "            self._loss = lossFunc[loss]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.A = mul(X, self.W) + self.B\n",
    "        self.O = self.activ(self.A)\n",
    "        return self.O\n",
    "    \n",
    "    def backward(self, rate, dLdO = None):\n",
    "        dLdA = None\n",
    "        if self.isOutLayer:\n",
    "            dLdA = self.dLdA\n",
    "            dLdO = self.dLdO\n",
    "        \n",
    "        if dLdA is None:\n",
    "            dLdA = self.Dactiv(self.A, dLdO, self.O)\n",
    "        \n",
    "        M = self.O.shape[0]\n",
    "        dLdB = np.average(dLdA, axis = 0)\n",
    "        dLdO, dLdW = Dmul(self.X, self.W, dLdA)\n",
    "        \n",
    "        self.B -= rate * dLdB\n",
    "        self.W -= rate * (dLdW / M + self.reg * self.W)\n",
    "\n",
    "        return dLdO\n",
    "    \n",
    "    def regLoss(self):\n",
    "        return self.reg * np.sum(self.W**2)/2\n",
    "    \n",
    "    def loss(self, y):\n",
    "        rst, self.dLdO, self.dLdA =  self._loss(self.O, y)\n",
    "        return rst\n",
    "\"\"\"\n",
    "    neural network class\n",
    "\"\"\"\n",
    "class simpleNN:\n",
    "    \"\"\"\n",
    "        init a neural network.\n",
    "        inputs(\n",
    "            layers:  A list of dict, i.e. \n",
    "                        [{   \n",
    "                             shape : (10, 20)\n",
    "                             activ : 'sigmoid',\n",
    "                         },\n",
    "                         {   \n",
    "                             shape : (20, 40)\n",
    "                             activ : 'sigmoid',\n",
    "                         },\n",
    "                         {   \n",
    "                             shape : (40, 10)\n",
    "                             activ : 'sigmoid',\n",
    "                             loss  : 'LSM',\n",
    "                             isOutLayer : True\n",
    "                             \n",
    "                         }]\n",
    "                    It specifies the node number and activation function of each layer. \n",
    "                    For output layer, the loss function should also be given.\n",
    "        )\n",
    "    \"\"\"\n",
    "    def __init__(self, layers):\n",
    "        ly = []\n",
    "        for l in layers:\n",
    "            ly.append(layer(l['shape'], l['activ'], l.get('isOutLayer', False), l.get('loss', None)))\n",
    "        self.layers = ly\n",
    "    \n",
    "    def forward(self, X, y = None):\n",
    "        loss = 0\n",
    "        for l in self.layers:\n",
    "            X = l.forward(X)\n",
    "            #loss += l.regLoss()\n",
    "        \n",
    "        if y is not None:\n",
    "            loss += self.layers[-1].loss(y)\n",
    "        \n",
    "        return X, loss\n",
    "\n",
    "    def backward(self, step):\n",
    "        dLdO = None\n",
    "        for l in reversed(self.layers):\n",
    "            dLdO = l.backward(step, dLdO)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, rate = 1e-3, epochs = 20, batchSz = 200, rateDecay = 1):\n",
    "        num = X_train.shape[0]\n",
    "        ite = int(max(1, num / batchSz))\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(num)\n",
    "            for i in range(ite):\n",
    "                ids = perm[i * batchSz: (i + 1) * batchSz]\n",
    "                X_batch = X_train[ids, :]\n",
    "                y_batch = y_train[ids]\n",
    "                \n",
    "                pred, loss = self.forward(X_batch, y_batch)\n",
    "                self.backward(rate)\n",
    "                \n",
    "            rate *= rateDecay\n",
    "            \n",
    "            print('------------------epoch:', epoch, '------------------' )\n",
    "            \n",
    "            pred, loss = self.forward(X_train, y_train)\n",
    "            print('train_accuracy: %.5f' % np.mean(np.argmax(pred, axis = 1) == y_train), ' loss: %.5f'% loss)\n",
    "            \n",
    "            pred, loss = self.forward(X_test, y_test)\n",
    "            print('test_accuracy : %.5f' % np.mean(np.argmax(pred, axis = 1) == y_test), ' loss: %.5f\\n'% loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    {\n",
    "        'shape': (784, 100),\n",
    "        'activ': 'relu',\n",
    "    },\n",
    "    {\n",
    "        'shape': (100, 100),\n",
    "        'activ': 'relu',\n",
    "    },\n",
    "    {\n",
    "        'shape': (100, 10),\n",
    "        'activ': 'softMax',\n",
    "        'isOutLayer' : True,\n",
    "        'loss' : 'softMaxLabelCrossEntropy'\n",
    "    }\n",
    "]\n",
    "snn = simpleNN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST-original', data_home='datasets/mnist')\n",
    "X = mnist['data'].astype(float)\n",
    "y = mnist['target'].astype(int)\n",
    "I = np.random.permutation(X.shape[0])\n",
    "X = X[I, :]\n",
    "y = y[I]\n",
    "\n",
    "X_train = X[:50000, :]\n",
    "y_train = y[:50000]\n",
    "X_test = X[50000:, :]\n",
    "y_test = y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------epoch: 0 ------------------\n",
      "train_accuracy: 0.80826  loss: 1.58909\n",
      "test_accuracy : 0.79310  loss: 1.79863\n",
      "\n",
      "------------------epoch: 1 ------------------\n",
      "train_accuracy: 0.82760  loss: 0.84931\n",
      "test_accuracy : 0.81525  loss: 0.98916\n",
      "\n",
      "------------------epoch: 2 ------------------\n",
      "train_accuracy: 0.84424  loss: 0.62283\n",
      "test_accuracy : 0.82930  loss: 0.73447\n",
      "\n",
      "------------------epoch: 3 ------------------\n",
      "train_accuracy: 0.86206  loss: 0.50067\n",
      "test_accuracy : 0.84895  loss: 0.59153\n",
      "\n",
      "------------------epoch: 4 ------------------\n",
      "train_accuracy: 0.87808  loss: 0.43555\n",
      "test_accuracy : 0.86190  loss: 0.52215\n",
      "\n",
      "------------------epoch: 5 ------------------\n",
      "train_accuracy: 0.88976  loss: 0.38440\n",
      "test_accuracy : 0.87440  loss: 0.46551\n",
      "\n",
      "------------------epoch: 6 ------------------\n",
      "train_accuracy: 0.89312  loss: 0.35875\n",
      "test_accuracy : 0.87870  loss: 0.43693\n",
      "\n",
      "------------------epoch: 7 ------------------\n",
      "train_accuracy: 0.90048  loss: 0.33104\n",
      "test_accuracy : 0.88690  loss: 0.40607\n",
      "\n",
      "------------------epoch: 8 ------------------\n",
      "train_accuracy: 0.90752  loss: 0.30744\n",
      "test_accuracy : 0.89390  loss: 0.37986\n",
      "\n",
      "------------------epoch: 9 ------------------\n",
      "train_accuracy: 0.90914  loss: 0.29650\n",
      "test_accuracy : 0.89420  loss: 0.36752\n",
      "\n",
      "------------------epoch: 10 ------------------\n",
      "train_accuracy: 0.91466  loss: 0.27870\n",
      "test_accuracy : 0.90075  loss: 0.34813\n",
      "\n",
      "------------------epoch: 11 ------------------\n",
      "train_accuracy: 0.91934  loss: 0.26369\n",
      "test_accuracy : 0.90480  loss: 0.32842\n",
      "\n",
      "------------------epoch: 12 ------------------\n",
      "train_accuracy: 0.92050  loss: 0.25804\n",
      "test_accuracy : 0.90855  loss: 0.32347\n",
      "\n",
      "------------------epoch: 13 ------------------\n",
      "train_accuracy: 0.92452  loss: 0.24555\n",
      "test_accuracy : 0.91030  loss: 0.30765\n",
      "\n",
      "------------------epoch: 14 ------------------\n",
      "train_accuracy: 0.92656  loss: 0.23845\n",
      "test_accuracy : 0.91280  loss: 0.29791\n",
      "\n",
      "------------------epoch: 15 ------------------\n",
      "train_accuracy: 0.93076  loss: 0.22593\n",
      "test_accuracy : 0.91835  loss: 0.28318\n",
      "\n",
      "------------------epoch: 16 ------------------\n",
      "train_accuracy: 0.93400  loss: 0.21687\n",
      "test_accuracy : 0.91965  loss: 0.27343\n",
      "\n",
      "------------------epoch: 17 ------------------\n",
      "train_accuracy: 0.93544  loss: 0.20963\n",
      "test_accuracy : 0.92005  loss: 0.26554\n",
      "\n",
      "------------------epoch: 18 ------------------\n",
      "train_accuracy: 0.93862  loss: 0.20087\n",
      "test_accuracy : 0.92530  loss: 0.25404\n",
      "\n",
      "------------------epoch: 19 ------------------\n",
      "train_accuracy: 0.93904  loss: 0.19818\n",
      "test_accuracy : 0.92480  loss: 0.25121\n",
      "\n",
      "------------------epoch: 20 ------------------\n",
      "train_accuracy: 0.94218  loss: 0.19044\n",
      "test_accuracy : 0.92825  loss: 0.24259\n",
      "\n",
      "------------------epoch: 21 ------------------\n",
      "train_accuracy: 0.94512  loss: 0.18409\n",
      "test_accuracy : 0.93070  loss: 0.23398\n",
      "\n",
      "------------------epoch: 22 ------------------\n",
      "train_accuracy: 0.94626  loss: 0.17822\n",
      "test_accuracy : 0.93285  loss: 0.22761\n",
      "\n",
      "------------------epoch: 23 ------------------\n",
      "train_accuracy: 0.94778  loss: 0.17333\n",
      "test_accuracy : 0.93325  loss: 0.22233\n",
      "\n",
      "------------------epoch: 24 ------------------\n",
      "train_accuracy: 0.95036  loss: 0.16816\n",
      "test_accuracy : 0.93555  loss: 0.21557\n",
      "\n",
      "------------------epoch: 25 ------------------\n",
      "train_accuracy: 0.95124  loss: 0.16394\n",
      "test_accuracy : 0.93660  loss: 0.21144\n",
      "\n",
      "------------------epoch: 26 ------------------\n",
      "train_accuracy: 0.95306  loss: 0.16048\n",
      "test_accuracy : 0.93750  loss: 0.20619\n",
      "\n",
      "------------------epoch: 27 ------------------\n",
      "train_accuracy: 0.95384  loss: 0.15709\n",
      "test_accuracy : 0.93915  loss: 0.20191\n",
      "\n",
      "------------------epoch: 28 ------------------\n",
      "train_accuracy: 0.95554  loss: 0.15258\n",
      "test_accuracy : 0.94010  loss: 0.19781\n",
      "\n",
      "------------------epoch: 29 ------------------\n",
      "train_accuracy: 0.95620  loss: 0.14989\n",
      "test_accuracy : 0.94095  loss: 0.19260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snn.train(X_train, y_train, X_test, y_test, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
